# @package _global_
defaults:
  - /evaluation/mblip_eval_train@module.evaluation
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
#  - override /module: mblip # not used by us but can be set to a file in configs/modules/
  - override /datamodule: mblip_train
  - override /callbacks: null
  - override /config_callbacks: trident.yaml
  - override /logger: wandb #csv #wandb.yaml

seed: 42
train: True
test_after_training: False

data_prefix: /media/gregor/DATA/projects/wuerzburg 
train_image_root: ${data_prefix}/mblip/data/pretrain/images 
train_data: ${data_prefix}/mblip/data
train_file: task_mix_v2_mt.json
xm3600_image_root: ${data_prefix}/iglue/datasets/Crossmodal3600/images
flickr_image_root: ${data_prefix}/iglue/datasets/flickr30k/flickr_images
gqa_image_root: ${data_prefix}/iglue/datasets/gqa/images

blip_checkpoint: ${data_prefix}/mblip/checkpoints/blip2-flant5xl-nolm.bin
blip_model: Salesforce/blip2-flan-t5-xl
llm:  bigscience/mt0-xl  #huggyllama/llama-7b # google/flan-t5-xl #bigscience/mt0-xl #bigscience/bloomz-7b1-mt
tokenizer_name: ${llm}
image_size: 224

trainer:
  num_sanity_val_steps: 0
  max_epochs: 1
  devices: 1
  precision: 16
  gradient_clip_val: 1.0
  accumulate_grad_batches: 16 #32 #16
  log_every_n_steps: 10
  val_check_interval: 0.1
#  strategy: deepspeed_stage_2
#  limit_train_batches: 10
#  limit_val_batches: 10


datamodule:
  dataloader_cfg:
    train:
      batch_size: 8 #4
      num_workers: 0
    val:
      batch_size: 4
      num_workers: 0

module:
  model:
    _target_: src.modules.modeling.mblip.mBLIPModule 
    blip_pretrained: ${blip_model}
    blip_pretrained_checkpoint: ${blip_checkpoint}
    lm_pretrained: ${llm}
    train_checkpoint: /example/path/to/runs/2023-05-13/11-46-24/checkpoints/0-8016.ckpt
    load_8bit: True
    freeze_vit: True
    freeze_qformer: False
    freeze_lm: True
    freeze_projection: False
    gradient_checkpoint: True
    random_init_projection: False
    compile: False
    use_lora: True
    lora_alpha: 16
    lora_r: 8
    lora_dropout: 0.05
    use_lora_vit: False
  optimizer:
    lr: 0.00005  #0.0005
    weight_decay: 0.1
  scheduler:
    _target_: transformers.optimization.get_cosine_schedule_with_warmup
    num_warmup_steps: 1000 #0.1

logger:
  csv:
    save_dir: ${hydra:runtime.output_dir}
  wandb:
    name: "instruct_${hydra:runtime.output_dir}_llm=${llm}_data=${train_file}"
    project: "mblip"


callbacks:
  model_checkpoint_on_epoch:
    _target_: src.tasks.vllm.checkpoint.mBLIPModelCheckpoint #lightning.pytorch.callbacks.ModelCheckpoint 
    freeze_vit: ${module.model.freeze_vit}
    freeze_qformer: ${module.model.freeze_qformer}
    freeze_lm: ${module.model.freeze_lm}
    freeze_projection: ${module.model.freeze_projection}
    lora: ${module.model.use_lora}
    lora_vit: ${module.model.use_lora_vit}
#    monitor: "" # name of the logged metric which determines when model is improving
    every_n_epochs: 1
    verbose: false
    save_top_k: -1 # -1 -> all models are saved
    save_last: false # additionaly always save model from last epoch
    dirpath: "${hydra:runtime.output_dir}/checkpoints/"
    auto_insert_metric_name: false
